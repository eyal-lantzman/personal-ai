from typing import Optional, Union
from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
from array import array
from typing import List, Literal
from services.internal.inference import EmbeddingsInference
from services.registry import REGISTRY
import base64

embeddings_router = APIRouter()

INFERENCE = EmbeddingsInference(REGISTRY)

class CreateEmbeddingsRequest(BaseModel):
    model: str
    input: Optional[Union[str, list[str]]]
    dimensions: Optional[int] = None #truncate_dim for Matryoshka Embeddings e.g. jinaai/jina-embeddings-v3 
    encoding_format: Optional[str] = "float" # can also be base64

class Usage(BaseModel):
    prompt_tokens: int
    """The number of tokens used by the prompt."""

    total_tokens: int
    """The total number of tokens used by the request."""

class Embedding(BaseModel):
    embedding: Union[str, List[float]]
    """The embedding vector, which is a list of floats.

    The length of vector depends on the model as listed in the
    [embedding guide](https://platform.openai.com/docs/guides/embeddings).
    """

    index: int
    """The index of the embedding in the list of embeddings."""

    object: Literal["embedding"]
    """The object type, which is always "embedding"."""


class CreateEmbeddingResponse(BaseModel):
    data: List[Embedding]
    """The list of embeddings generated by the model."""

    model: str
    """The name of the model used to generate the embedding."""

    object: Literal["list"]
    """The object type, which is always "list"."""

    usage: Usage
    """The usage information for the request."""


@embeddings_router.post("/")
async def embeddings_create(request: CreateEmbeddingsRequest) -> CreateEmbeddingResponse:

    try:
        if not INFERENCE.ready or INFERENCE.model_id != request.model:
            INFERENCE.reset()
            INFERENCE.serve(request.model)

        encode_kwargs = dict()
        if request.dimensions is not None:
            encode_kwargs["truncate_dim"] = request.dimensions

        sentences = list()
        if isinstance(request.input, str):
            sentences.append(request.input)
        else:
            sentences.extend(request.input)

        embeddings = INFERENCE.encode(sentences, **encode_kwargs)

        emedding_list = list[Embedding]()
        response = CreateEmbeddingResponse(
            object="list", 
            data=emedding_list, 
            model=request.model, 
            usage=Usage(prompt_tokens=len(request.input), total_tokens=len(request.input)+sum([len(x) for x in embeddings]))
        )

        i = 0
        for embedding in embeddings:
            floats = embedding.tolist()
            if request.encoding_format == "base64":
                floats = base64.b64encode(array("f", floats)).decode("utf-8")
            response.data.append(
                Embedding(
                    object="embedding", 
                    index=i, 
                    embedding= floats)
            )
            i += 1

        return response

    except ValueError as e:
        raise HTTPException(400, detail=str(e))
    except NotImplementedError as e:
        raise HTTPException(404, detail=str(e))